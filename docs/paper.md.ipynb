{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    import jupyter, notebook, notebook as üìì, IPython, pidgy, pidgy as üê¶, mistune as markdown, mistune as M‚¨áÔ∏è, IPython as python, ast, jinja2 as template, importnb as _import_, doctest, pathlib, graphviz    \n",
    "    from pidgy import *\n",
    "    from pidgy.tests import interactive as testing\n",
    "    shell = IPython.get_ipython()\n",
    "    import pandas as üêº\n",
    "    √ò = __name__ == '__main__'\n",
    "\n",
    "    with üê¶.pidgyLoader():\n",
    "        import pidgy.reuse, pidgy.conversion, pidgy.cli, pidgy.tests.readme, appendix, intro, test_pidgin_syntax\n",
    "\n",
    "    \n",
    "    if √ò:\n",
    "        input_formats = !pandoc --list-input-formats\n",
    "        input_formats = {x.split('_')[0] for x in input_formats}\n",
    "        kernels = üêº.read_html(appendix.get('https://github.com/jupyter/jupyter/wiki/Jupyter-kernels'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".jp-mod-presentationMode {\n",
       "    --jp-notebook-padding: 0;\n",
       "}\n",
       ".jp-RenderedHTMLCommon pre code {\n",
       "    opacity: 0.25;\n",
       "}\n",
       ".jp-Placeholder-content .jp-MoreHorizIcon {\n",
       "    background-size: 32px;\n",
       "}\n",
       "</style><style>\n",
       ".jp-mod-presentationMode .jp-SideBar,\n",
       ".jp-mod-presentationMode #jp-top-panel {\n",
       "    opacity: 0.0;\n",
       "    transition: all 0.2s;\n",
       "}\n",
       ".jp-mod-presentationMode .jp-SideBar:hover,\n",
       ".jp-mod-presentationMode #jp-top-panel:hover {\n",
       "    opacity: 0.9;\n",
       "    transition: all 0.2s;\n",
       "}</style><style>\n",
       ".jp-mod-presentationMode.jp-ApplicationShell,\n",
       ".jp-mod-presentationMode .p-TabBar-content{\n",
       "    background-color: var(--jp-layout-color0);\n",
       "}\n",
       "</style><style>\n",
       ".jp-mod-presentationMode .p-DockPanel-widget,\n",
       ".jp-mod-presentationMode #jp-left-stack{\n",
       "    border-color: transparent;\n",
       "}\n",
       ".jp-mod-presentationMode .jp-Toolbar-item,\n",
       ".jp-mod-presentationMode .jp-Toolbar {\n",
       "    opacity: 0.1;\n",
       "    transition: all 0.2s;\n",
       "}\n",
       ".jp-mod-presentationMode .jp-Toolbar-item:hover,\n",
       ".jp-mod-presentationMode .jp-Toolbar:hover {\n",
       "    opacity: 0.9;\n",
       "    transition: all 0.2s;\n",
       "}\n",
       "\n",
       ".jp-mod-presentationMode .jp-InputArea {\n",
       "    flex-direction: column;\n",
       "}\n",
       "\n",
       "</style><style>\n",
       ".jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt, \n",
       ".jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {\n",
       "    flex: 0 0 2rem !important;\n",
       "    opacity: 0;\n",
       "}\n",
       ".jp-mod-presentationMode .jp-Notebook .jp-Cell.jp-mod-active .jp-OutputPrompt,\n",
       ".jp-mod-presentationMode .jp-Notebook .jp-Cell.jp-mod-active .jp-OutputPrompt {\n",
       "    opacity: 0.5;\n",
       "}\n",
       ".jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt, \n",
       ".jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt\n",
       "\n",
       ".jp-mod-presentationMode hr {\n",
       "    opacity: 0.1;\n",
       "}\n",
       "</style>\n",
       "    <style>\n",
       "    .jp-TableOfContents-content h1, \n",
       "    .jp-TableOfContents-content h2 {\n",
       "        margin-bottom: var(--jp-ui-font-size0);\n",
       "    }\n",
       "    </style>\n",
       "    \n",
       "    <style>\n",
       "    .jp-mod-presentationMode {\n",
       "        --jp-content-heading-line-height: 1.25 !important;\n",
       "    }\n",
       "    </style>\n",
       "    \n",
       "    <style>\n",
       "    .jp-mod-presentationMode #jp-main-status-bar {\n",
       "        opacity: 0.06;\n",
       "        transition: all 0.2s;\n",
       "    }\n",
       "    .jp-mod-presentationMode #jp-main-status-bar:hover {\n",
       "        opacity: 0.8;\n",
       "        transition: all 0.2s;\n",
       "    }\n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "    üê¶.__style__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-03T23:04:44.645258",
       "modules": [],
       "names": [],
       "start_time": "2020-02-03T23:04:44.632177"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    üê¶.__style__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# `pidgy` programming\n",
       "\n",
       "\n",
       "\n",
       "`üê¶,pidgy` programming is a fun and expressive style of literate computing\n",
       "designed for writing nonfiction literate in `jupyter` computational `üìì`s.\n",
       "It presents a `M‚¨áÔ∏è`-forward style of programming where authors\n",
       "codevelop code and narrative. \n",
       "`pidgy`programs are intermediate documents \n",
       "that can be read, written, and formally tested.\n",
       "\n",
       "\n",
       "\n",
       "pidgy programming is a style of literate computing,\n",
       "it is an expressive way of writing program\n",
       "\n",
       "\n",
       "pidgy programming is concerned with scientific literacy, the general abilities to read & write scientific literature, and the implicit pleasure of interactively\n",
       "composing modern literate programs.\n",
       "Access to commodity computing infrastructures have affected \n",
       "the forms of scientific information architecture and \n",
       "scientific literature.\n",
       "Programming languages represent novel forms\n",
       "that express heuristics that are explicitly reusable, they fill the voids where \n",
       "the superior code of language fails to communicate phenomenon.\n",
       "Scientific literature written in `pidgy` accept all \n",
       "languages equitably, it assigns language the role of communicating\n",
       "computational thought in fluid combinations of human or machine logic.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-03T22:29:37.310116",
       "modules": [],
       "names": [],
       "start_time": "2020-02-03T22:29:37.210049"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{appendix.exports(intro)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Literary code & coded literature\n",
       "\n",
       "\n",
       "\n",
       "    \"\"\"\n",
       "literate `pidgy` programs have two intents:\n",
       "1. be literature that acts as a program.\n",
       "2. be a program that acts as literature.\n",
       "\n",
       "In this approach, programming languages merge with natural language\n",
       "to communicate thought.  As a whole, these documents encapsulate\n",
       "unstructured languages that communicate with combinations of human \n",
       "and machine logic.  \n",
       "\n",
       "    \"\"\";\n",
       "\n",
       "\n",
       "\n",
       "    class pidgyLoader(__import__('importnb').Notebook): \n",
       "        \"\"\"\n",
       "The `pidgyLoader` includes `pidgy` documents in Python's import system.\n",
       "        \n",
       "        \"\"\"\n",
       "        extensions = \".md.ipynb\".split()\n",
       "        \"\"\"\n",
       "Regardless of intent, `pidgy` programs should be reusable in other programs.\n",
       "`pidgy` programs are identified by the composite file extension `\".md.ipynb\"`.\n",
       "The choice is file extension is made because `pidgy` programs \n",
       "designed primarly for literate programming in `jupyter` `notebook`s - \n",
       "that use the `\".ipynb\"` suffix - \n",
       "with `markdown` as the document language and `IPython` as the glue programming language.\n",
       "        \n",
       "        \"\"\"\n",
       "        def code(self, str): \n",
       "            \"\"\"\n",
       "Appply the pidgy transformers.\n",
       "        \n",
       "            \"\"\"\n",
       "            return ''.join(__import__('pidgy').translate.pidgy.transform_cell(str))\n",
       "\n",
       "\n",
       "\n",
       "## Authoring reusable documents.\n",
       "\n",
       "\n",
       "\n",
       "    \"\"\"\n",
       "For documents to be reused as modules, they must restart and run all.\n",
       "\n",
       "A benefit of this approach is that the documents can be tested.\n",
       "\n",
       "    \"\"\";\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-03T22:29:37.531530",
       "modules": [],
       "names": [],
       "start_time": "2020-02-03T22:29:37.391446"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{appendix.exports(pidgy.reuse)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deriving files from `pidgy` documents.\n",
       "\n",
       "\n",
       "\n",
       "There are numerous tools that use the `notebook` format as an intermediate formats\n",
       "for different documents.\n",
       "\n",
       "The original literate programming used latex as the sole export format\n",
       "where as the notebook recognizes quite a few formats:\n",
       "    \n",
       "<details><summary><code>nbconvert</code> can generate <b>12</b> different formats from the files that abide the <code>nbformat</code>\n",
       "schema.</summary>\n",
       "<ul><li>html</li>\n",
       "<li>python</li>\n",
       "<li>asciidoc</li>\n",
       "<li>notebook</li>\n",
       "<li>slides</li>\n",
       "<li>pdf</li>\n",
       "<li>latex</li>\n",
       "<li>selectLanguage</li>\n",
       "<li>custom</li>\n",
       "<li>script</li>\n",
       "<li>markdown</li>\n",
       "<li>rst</li>\n",
       "</ul>\n",
       "</details>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "    class pidgyTranslate(nbconvert.preprocessors.Preprocessor):\n",
       "        def preprocess_cell(self, cell, resources, index, ):\n",
       "            import pidgy\n",
       "            if cell['cell_type'] == 'code':\n",
       "                cell['source'] = pidgy.imports.pidgy.transform_cell(''.join(cell['source']))\n",
       "            return cell, resources\n",
       "\n",
       "\n",
       "\n",
       "    class pidgyNormalize(nbconvert.preprocessors.Preprocessor):\n",
       "Untangle a pidgy notebook into a normalized notebook that explicitly sepearting code and markdown cells.\n",
       "A normalized notebook can be imported by importnb.\n",
       "        \n",
       "        def preprocess(self, nb, resources):\n",
       "            new, tokens = nbformat.v4.new_notebook(), []\n",
       "            for cell in nb.cells:\n",
       "                for token in tokenizer.parse(''.join(cell.source)) if cell.cell_type == 'code' else [{'type': 'paragraph', 'text': ''.join(cell.source)}]:\n",
       "                    new.cells.append((\n",
       "                        nbformat.v4.new_code_cell if token['type'] == 'code' else nbformat.v4.new_markdown_cell\n",
       "                    )(token['text'].splitlines(True)))\n",
       "            return nb, resources\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-03T22:29:37.656451",
       "modules": [],
       "names": [],
       "start_time": "2020-02-03T22:29:37.557720"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{appendix.exports(pidgy.conversion)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Literature as the test\n",
       "\n",
       "Literate `pidgy` programs accomodate varities of syntaxes and opinions.\n",
       "`pidgy` takes the opinion that everything that is code is computable.\n",
       "Interactive doctests are tested.\n",
       "\n",
       "\n",
       "\n",
       "`pidgy` provides a `pytest` plugin that works only on `\".md.ipynb\"` files.\n",
       "The `pidgy.kernel` works directly with `nbval`, install the python packkage and use the --nbval flag.\n",
       "`pidgy` uses features from `importnb` to support standard tests discovery, \n",
       "and `doctest` discovery across all strings.\n",
       "\n",
       "Still working on coverage.\n",
       "\n",
       "\n",
       "\n",
       "    class pidgyModule(importnb.utils.pytest_importnb.NotebookModule):\n",
       "The `pidgyModule` permits standard test discovery in notebooks.\n",
       "Functions beginning with `\"test_\"` indicate test functions.\n",
       "\n",
       "        loader = pidgy.imports.pidgyLoader\n",
       "\n",
       "\n",
       "\n",
       "    class pidgyTests(importnb.utils.pytest_importnb.NotebookTests):\n",
       "        modules = pidgyModule,\n",
       "\n",
       "    pytest_collect_file = pidgyTests.__call__\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-03T22:29:37.808687",
       "modules": [],
       "names": [],
       "start_time": "2020-02-03T22:29:37.717356"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{appendix.exports(pidgy.tests.readme)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Building the `pidgy` extension\n",
       "\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "The `pidgy` implementation uses the `IPython` configuration\n",
       "and extension system to modify the interactive computing expierence\n",
       "in `jupyter` notebooks.\n",
       "        \n",
       "        imports.load_ipython_extension(shell)\n",
       "1. The primary function of `pidgy` is that it `imports` `markdown` as formal language for \n",
       "programming multiobjective literate programs.  `imports` focuses on the indentification of\n",
       "`\"code\" and not\"code\"` that become python code.\n",
       "\n",
       "        testing.load_ipython_extension(shell) \n",
       "2. The `pidgy` specification promotes strong intertextuality between `\"code\" and not\"code\"` \n",
       "objects in a program.  `testing` reinforces that efficacy of the `\"code\"` using\n",
       "documentation tests of `doctest and \"inline\"+\"code\"`.  `pidgy` uses the narrative a formal \n",
       "test for the program.  These tests are executed interactively to ensure the veracity of \n",
       "`\"code\"` signs in the narrative.\n",
       "\n",
       "        exports.load_ipython_extension(shell)\n",
       "3. Literate computing in `pidgy` allows incremental development of `\"code\"` and the co-development of the documentation.\n",
       "`pidgy` interprets the `input` `\"code\"` as a `display`.  `pidgy` uses a `template` language to transclude\n",
       "`object`s from code \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-03T22:29:38.625623",
       "modules": [],
       "names": [],
       "start_time": "2020-02-03T22:29:38.544573"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "{{appendix.exports(pidgy.extension)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"461pt\" height=\"170pt\"\n",
       " viewBox=\"0.00 0.00 461.00 170.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 166)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-166 457,-166 457,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_pidgy</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"8,-8 8,-154 262,-154 262,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-138.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">new school</text>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_web</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"270,-8 270,-154 445,-154 445,-8 270,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"357.5\" y=\"-138.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">old school</text>\n",
       "</g>\n",
       "<!-- pidgy -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>pidgy</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"126\" cy=\"-106\" rx=\"30.0035\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"126\" y=\"-101.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">pidgy</text>\n",
       "</g>\n",
       "<!-- PYTHON -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>PYTHON</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"62\" cy=\"-34\" rx=\"45.9459\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"62\" y=\"-29.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">PYTHON</text>\n",
       "</g>\n",
       "<!-- pidgy&#45;&gt;PYTHON -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>pidgy&#45;&gt;PYTHON</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M111.7963,-90.0209C103.6382,-80.843 93.2174,-69.1196 84.0192,-58.7716\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"86.5409,-56.3403 77.2813,-51.1915 81.309,-60.9909 86.5409,-56.3403\"/>\n",
       "</g>\n",
       "<!-- MARKDOWN -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>MARKDOWN</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"190\" cy=\"-34\" rx=\"64.296\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"190\" y=\"-29.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">MARKDOWN</text>\n",
       "</g>\n",
       "<!-- pidgy&#45;&gt;MARKDOWN -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>pidgy&#45;&gt;MARKDOWN</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M140.2037,-90.0209C148.2444,-80.975 158.4832,-69.4564 167.583,-59.2191\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"170.2367,-61.5019 174.2644,-51.7025 165.0048,-56.8513 170.2367,-61.5019\"/>\n",
       "</g>\n",
       "<!-- WEB -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>WEB</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"364\" cy=\"-106\" rx=\"29.5104\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"364\" y=\"-101.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">WEB</text>\n",
       "</g>\n",
       "<!-- PASCAL -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>PASCAL</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"321\" cy=\"-34\" rx=\"43.4111\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"321\" y=\"-29.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">PASCAL</text>\n",
       "</g>\n",
       "<!-- WEB&#45;&gt;PASCAL -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>WEB&#45;&gt;PASCAL</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M353.8096,-88.937C348.7281,-80.4284 342.4682,-69.9468 336.7833,-60.4279\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"339.6987,-58.4833 331.5663,-51.6924 333.6889,-62.0725 339.6987,-58.4833\"/>\n",
       "</g>\n",
       "<!-- TEX -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>TEX</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"410\" cy=\"-34\" rx=\"27.0958\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"410\" y=\"-29.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">TEX</text>\n",
       "</g>\n",
       "<!-- WEB&#45;&gt;TEX -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>WEB&#45;&gt;TEX</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M374.9014,-88.937C380.5155,-80.1496 387.4741,-69.2579 393.7112,-59.4956\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"396.7424,-61.2519 399.1769,-50.9405 390.8435,-57.4831 396.7424,-61.2519\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11415e208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/markdown": [
       "## programming in `markdown and python` \n",
       "[üìì](translate.ipynb)\n",
       "\n",
       "\n",
       "    def load_ipython_extension(shell):\n",
       "        \"\"\"\n",
       "The `pidgy` `load_ipython_extension`'s primary function transforms the `jupyter`\n",
       "`notebook`s into a literate computing interfaces.\n",
       "`markdown` becomes the primary plain-text format for submitting code,\n",
       "and the `markdown` is translated to `python` source code\n",
       "before compilation.\n",
       "The implementation configures the appropriate\n",
       "features of the `IPython.InteractiveShell` to accomodate\n",
       "the interactive literate programming experience.\n",
       "\n",
       "In this section, we'll implement a `shell.input_transformer_manager`\n",
       "that handles the logical translation of `markdown` to `python`.\n",
       "The translation maintains the source line numbers and \n",
       "normalizes the narrative relative to the source code.  Consequently,\n",
       "introduces new syntaxes at the interfaces between `markdown and python`.\n",
       "\n",
       "        \"\"\"\n",
       "        pidgy_transformer = pidgyTransformer()        \n",
       "        shell.input_transformer_manager = pidgy_transformer\n",
       "        \n",
       "        \"\"\"\n",
       "`IPython` provides configurable interactive `shell` properties.  Some of the configurable properties\n",
       "control how `input` code is translated into valid source code. \n",
       "The `pidgy` translation is managed by a custom `IPython.core.inputtransformer2.TransformerManager`.\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformer_manager\n",
       "        <...pidgyTransformer object...>\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "\n",
       "The `shell.input_transformer_manager` applies string transformations to clean up the `input`\n",
       "to be valid `python`.  There are three stages of line of transforms.\n",
       "\n",
       "1. Cleanup transforms that operate on the entire cell `input`.\n",
       "\n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformers_cleanup\n",
       "        [<...leading_empty_lines...>, <...leading_indent...>, <...PromptStripper...>, ...]\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        \n",
       "2. Line transforms that are applied the cell `input` with split lines. \n",
       "This is where `IPython` introduces their bespoke cell magic syntaxes.\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformer_manager.line_transforms\n",
       "        [...<...cell_magic...>...]\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "        \n",
       "3. Token transformers that look for specific tokens at the like level.  `IPython`'s default\n",
       "behavior introduces new symbols into the programming language.\n",
       "\n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.input_transformer_manager.token_transformers\n",
       "        [<...MagicAssign...SystemAssign...EscapedCommand...HelpEnd...>]\n",
       "        \n",
       "        \"\"\"\"\"\"\n",
       "\n",
       "After all of the `input` transformations are complete, the `input` should be valid source that `ast.parse, compile or shell.compile` \n",
       "may accept.\n",
       "\n",
       "        \"\"\"\"\"\"\n",
       "        >>> shell.ast_transformers\n",
       "        [...]\n",
       "        \n",
       "        \"\"\"\n",
       "\n",
       "        if not any(x for x in shell.ast_transformers if isinstance(x, ReturnYield)):\n",
       "            shell.ast_transformers.append(ReturnYield())\n",
       "\n",
       "\n",
       "\n",
       "    class pidgyTransformer(IPython.core.inputtransformer2.TransformerManager):\n",
       "        def pidgy_transform(self, cell: str) -> str: \n",
       "            tokens = self.tokenizer.parse(''.join(cell))\n",
       "            return self.tokenizer.untokenize(tokens)\n",
       "        \n",
       "        def pidgy_cleanup(self, cell: str) -> list: \n",
       "            return self.pidgy_transform(cell).splitlines(True)\n",
       "        \n",
       "        def __init__(self, *args, **kwargs):\n",
       "            super().__init__(*args, **kwargs)\n",
       "            self.tokenizer = Tokenizer()\n",
       "            self.cleanup_transforms.insert(0, self.pidgy_cleanup)\n",
       "            self.line_transforms.append(demojize)\n",
       "\n",
       "        def pidgy_magic(self, *text): \n",
       "            \"\"\"Expand the text to tokens to tokens and \n",
       "            compact as a formatted `\"python\"` code.\"\"\"\n",
       "            return IPython.display.Code(self.pidgy_transform(''.join(text)), language='python')\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    import ast\n",
       "    class ReturnYield(ast.NodeTransformer):\n",
       "        def visit_FunctionDef(self, node): return node\n",
       "        visit_AsyncFunctionDef = visit_FunctionDef\n",
       "        def visit_Return(self, node):\n",
       "            replace = ast.parse('''__import__('IPython').display.display()''').body[0]\n",
       "            replace.value.args = node.value.elts if isinstance(node.value, ast.Tuple) else [node.value]\n",
       "            return ast.copy_location(replace, node)\n",
       "\n",
       "        def visit_Expr(self, node):\n",
       "            if isinstance(node.value, (ast.Yield, ast.YieldFrom)):  return ast.copy_location(self.visit_Return(node.value), node)\n",
       "            return node\n",
       "        \n",
       "        visit_Expression = visit_Expr\n",
       "\n",
       "\n",
       "\n",
       "    import mistune as markdown, textwrap, __main__, IPython, typing, re, IPython, nbconvert, ipykernel, doctest, ast\n",
       "    __all__ = 'pidgy',\n",
       "\n",
       "\n",
       "\n",
       "    class Tokenizer(markdown.BlockLexer):\n",
       "            \"\"\"\n",
       "### Tokenizer\n",
       "\n",
       "<details>\n",
       "<summary>Tokenize `input` text into `\"code\" and not \"code\"` tokens that will be translated into valid `python` source.</summary>\n",
       "        \n",
       "            \"\"\"\n",
       "            class grammar_class(markdown.BlockGrammar):\n",
       "                doctest = doctest.DocTestParser._EXAMPLE_RE\n",
       "\n",
       "            def parse(self, text: str, default_rules=None) -> typing.List[dict]:\n",
       "                if not self.depth: self.tokens = []\n",
       "                with self: tokens = super().parse(whiten(text), default_rules)\n",
       "                if not self.depth: tokens = self.compact(text, tokens)\n",
       "                return tokens\n",
       "\n",
       "            def parse_doctest(self, m): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "\n",
       "            def parse_fences(self, m):\n",
       "                if m.group(2): self.tokens.append({'type': 'paragraph', 'text': m.group(0)})\n",
       "                else: super().parse_fences(m)\n",
       "\n",
       "            def parse_hrule(self, m):\n",
       "                self.tokens.append({'type': 'hrule', 'text': m.group(0)})\n",
       "\n",
       "            def compact(self, text, tokens):\n",
       "                \"\"\"Combine non-code tokens into contiguous blocks.\"\"\"\n",
       "                compacted = []\n",
       "                while tokens:\n",
       "                    token = tokens.pop(0)\n",
       "                    if 'text' not in token: continue\n",
       "                    else: \n",
       "                        if not token['text'].strip(): continue\n",
       "                        block, body = token['text'].splitlines(), \"\"\n",
       "                    while block:\n",
       "                        line = block.pop(0)\n",
       "                        if line:\n",
       "                            before, line, text = text.partition(line)\n",
       "                            body += before + line\n",
       "                    if token['type']=='code':\n",
       "                        compacted.append({'type': 'code', 'lang': None, 'text': body})\n",
       "                    else:\n",
       "                        if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                            compacted[-1]['text'] += body\n",
       "                        else: compacted.append({'type': 'paragraph', 'text': body})\n",
       "                if compacted and compacted[-1]['type'] == 'paragraph':\n",
       "                    compacted[-1]['text'] += text\n",
       "                elif text.strip():\n",
       "                    compacted.append({'type': 'paragraph', 'text': text})\n",
       "                return compacted\n",
       "\n",
       "            depth = 0\n",
       "            def __enter__(self): self.depth += 1\n",
       "            def __exit__(self, *e): self.depth -= 1\n",
       "\n",
       "            def untokenize(self, tokens: œÑ.List[dict], source: str = \"\"\"\"\"\", last: int =0) -> str:\n",
       "                INDENT = indent = base_indent(tokens) or 4\n",
       "                for i, token in enumerate(tokens):\n",
       "                    object = token['text']\n",
       "                    if token and token['type'] == 'code':\n",
       "                        if object.lstrip().startswith(FENCE):\n",
       "\n",
       "                            object = ''.join(''.join(object.partition(FENCE)[::2]).rpartition(FENCE)[::2])\n",
       "                            indent = INDENT + num_first_indent(object)\n",
       "                            object = textwrap.indent(object, INDENT*SPACE)\n",
       "\n",
       "                        if object.lstrip().startswith(MAGIC):  ...\n",
       "                        else: indent = num_last_indent(object)\n",
       "                    elif not object: ...\n",
       "                    else:\n",
       "                        object = textwrap.indent(object, indent*SPACE)\n",
       "                        for next in tokens[i+1:]:\n",
       "                            if next['type'] == 'code':\n",
       "                                next = num_first_indent(next['text'])\n",
       "                                break\n",
       "                        else: next = indent       \n",
       "                        Œî = max(next-indent, 0)\n",
       "\n",
       "                        if not Œî and source.rstrip().rstrip(CONTINUATION).endswith(COLON): \n",
       "                            Œî += 4\n",
       "\n",
       "                        spaces = num_whitespace(object)\n",
       "                        \"what if the spaces are ling enough\"\n",
       "                        object = object[:spaces] + Œî*SPACE+ object[spaces:]\n",
       "                        if not source.rstrip().rstrip(CONTINUATION).endswith(QUOTES): \n",
       "                            object = quote(object)\n",
       "                    source += object\n",
       "\n",
       "                for token in reversed(tokens):\n",
       "                    if token['text'].strip():\n",
       "                        if token['type'] != 'code': \n",
       "                            source = source.rstrip() + SEMI\n",
       "                        break\n",
       "\n",
       "                return source \n",
       "            \n",
       "    for x in \"default_rules footnote_rules list_rules\".split():\n",
       "        setattr(Tokenizer, x, list(getattr(Tokenizer, x)))\n",
       "        getattr(Tokenizer, x).insert(getattr(Tokenizer, x).index('block_code'), 'doctest')\n",
       "        \n",
       "    ...\n",
       "    \"\"\"\n",
       "</details>&nbsp;\n",
       "\n",
       "    \"\"\"\n",
       "    pidgy = pidgyTransformer()\n",
       "\n",
       "\n",
       "A potential outcome of a `pidgy` program is reusable code. \n",
       "\n",
       "Import pidgy notebooks as modules.\n",
       "\n",
       "\n",
       "    class pidgyLoader(__import__('importnb').Notebook): \n",
       "        extensions = \".ipynb .md.ipynb\".split()\n",
       "        def code(self, str): return ''.join(pidgy.transform_cell(str))\n",
       "\n",
       "\n",
       "\n",
       "    class pidgyPreprocessor(nbconvert.preprocessors.Preprocessor):\n",
       "        def preprocess_cell(self, cell, resources, index, ):\n",
       "            if cell['cell_type'] == 'code':\n",
       "                cell['source'] = pidgy_transformer.transform_cell(''.join(cell['source']))\n",
       "            return cell, resources\n",
       "\n",
       "\n",
       "\n",
       "    graphviz.Source(\n",
       "digraph{rankdir=UD \n",
       "subgraph cluster_pidgy {label=\"new school\" pidgy->{PYTHON MARKDOWN}}\n",
       "subgraph cluster_web {label=\"old school\" WEB->{PASCAL TEX} }}\n",
       "    \n",
       "    )"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-03T22:29:38.927033",
       "modules": [],
       "names": [],
       "start_time": "2020-02-03T22:29:38.730285"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## programming in `markdown and python` \n",
    "[üìì]({{pathlib.Path(translate.__file__).name}})\n",
    "\n",
    "\n",
    "{{appendix.exports(translate)}}\n",
    "\n",
    "    graphviz.Source(\n",
    "digraph{rankdir=UD \n",
    "subgraph cluster_pidgy {label=\"new school\" pidgy->{PYTHON MARKDOWN}}\n",
    "subgraph cluster_web {label=\"old school\" WEB->{PASCAL TEX} }}\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## testing `\"code\"` in the `markdown` narrative.\n",
       "[üìî](interactive.md.ipynb)\n",
       "\n",
       "In literate programs, `\"code\"` is deeply entangled with the narrative.\n",
       "`\"code\"` object can signify meaning and can be validated through testing.\n",
       "`python` introduced the `doctest` literate programming convention that indicates some text in a narrative can be tested.\n",
       "`pidgy` extends the `doctest` opinion to the inline markdown code.\n",
       "Each time a `pidgy` cell is executed, the `doctest`s and inline code are executed ensuring that\n",
       "any code in a `pidgy` program is valid.\n",
       "\n",
       "\n",
       "\n",
       "    def post_run_cell(result):\n",
       "        result.runner = test_markdown_string(result.info.raw_cell, IPython.get_ipython(), False, doctest.ELLIPSIS)\n",
       "\n",
       "    def load_ipython_extension(shell): \n",
       "        unload_ipython_extension(shell)\n",
       "        shell.events.register('post_run_cell', post_run_cell)\n",
       "\n",
       "\n",
       "\n",
       "    import doctest, contextlib, mistune as markdown, re, ast, __main__, IPython, operator\n",
       "    shell = IPython.get_ipython()\n",
       "\n",
       "\n",
       "`test_markdown_string` extends the standard python `doctest` tools \n",
       "to inline code objects written in markdown.  \n",
       "This approach compliments are markdown forward programming language to test\n",
       "intertextual references between code and narrative.\n",
       "\n",
       "\n",
       "    INLINE = re.compile(\n",
       "        markdown.InlineGrammar.code\n",
       "        .pattern[1:]\n",
       "        .replace('[\\s\\S]*', '?P<source>[\\s\\S]+')\n",
       "        .replace('+)\\s*', '{1,2})(?P<indent>\\s{0})'), \n",
       "    )\n",
       "\n",
       "\n",
       "    (TICK,), SPACE = '`'.split(), ' '\n",
       "\n",
       "\n",
       "\n",
       "    def test_markdown_string(str, shell=shell, verbose=False, compileflags=None):\n",
       "        globs, filename = shell.user_ns, F\"In[{shell.last_execution_result.execution_count}]\"\n",
       "        runner = doctest.DocTestRunner(verbose=verbose, optionflags=compileflags)  \n",
       "        parsers = DocTestParser(runner), InlineDoctestParser(runner)\n",
       "        parsers = {\n",
       "            parser: doctest.DocTestFinder(verbose, parser).find(str, filename) for parser in parsers\n",
       "        }\n",
       "        examples = sum([test.examples for x in parsers.values() for test in x], [])\n",
       "        examples.sort(key=operator.attrgetter('lineno'))\n",
       "        with ipython_compiler(shell):\n",
       "            for example in examples:\n",
       "                for parser, value in parsers.items():\n",
       "                    for value in value:\n",
       "                        if example in value.examples:\n",
       "                            with parser:\n",
       "                                runner.run(doctest.DocTest(\n",
       "                                    [example], globs, value.name, filename, example.lineno, value.docstring\n",
       "                                ), compileflags=compileflags, clear_globs=False)\n",
       "        shell.log.info(F\"In[{shell.last_execution_result.execution_count}]: {runner.summarize()}\")\n",
       "        return runner\n",
       "\n",
       "\n",
       "\n",
       "    @contextlib.contextmanager\n",
       "    def ipython_compiler(shell):\n",
       "        def compiler(input, filename, symbol, *args, **kwargs):\n",
       "            nonlocal shell\n",
       "            return shell.compile(\n",
       "                ast.Interactive(\n",
       "                    body=shell.transform_ast(\n",
       "                        shell.compile.ast_parse(input)\n",
       "                    ).body\n",
       "                ),\n",
       "                F\"In[{shell.last_execution_result.execution_count}]\",\n",
       "                \"single\",\n",
       "            )\n",
       "\n",
       "        yield setattr(doctest, \"compile\", compiler)\n",
       "        try:\n",
       "            doctest.compile = compile\n",
       "        except:\n",
       "            ...\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-03T22:29:39.050627",
       "modules": [],
       "names": [],
       "start_time": "2020-02-03T22:29:38.935246"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## testing `\"code\"` in the `markdown` narrative.\n",
    "[üìî]({{pathlib.Path(testing.__file__).name}})\n",
    "\n",
    "{{appendix.exports(testing)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Weaving the `markdown` to a rich display.\n",
       "[üìó](outputs.md.ipynb)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-03T22:30:26.786526",
       "modules": [],
       "names": [],
       "start_time": "2020-02-03T22:30:26.696921"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Weaving the `markdown` to a rich display.\n",
    "[üìó]({{pathlib.Path(outputs.__file__).name}})\n",
    "\n",
    "{{appendix.exports(outputs)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## `pidgy` metasyntax at language interfaces.\n",
       "[üìó](test_pidgin_syntax.md.ipynb)\n",
       "\n",
       "The combinations of document, programming, and templating languages\n",
       "provides unique syntaxes as the interfaces.\n",
       "\n",
       "This is a code string\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "`pidgy` programming is a `markdown`-forward approach to programming,\n",
       "it extends computational to interactive literate programming environment.\n",
       "One feature `markdown` uses to identify `markdown.BlockGrammar.block_code`\n",
       "is indented code.\n",
       "`pidgy` starts here, all cells are `markdown` forward, and code is identified as indented code.\n",
       "\n",
       "            \"This is a code string\"\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "### Code fences\n",
       "\n",
       "Some folks may prefer code fences and they may be used without a language specified.\n",
       "\n",
       "\n",
       "```\n",
       "\"This is code\"\n",
       "```\n",
       "\n",
       "```python\n",
       "\"This is not code.\"\n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "    class DocStrings:\n",
       "### Docstrings\n",
       "\n",
       "\n",
       "    >>> assert DocStrings.__doc__.startswith('### Docstrings')\n",
       "    >>> DocStrings.function_docstring.__doc__\n",
       "    '`DocStrings.function_docstring`s appear as native docstrings, ...'\n",
       "\n",
       "\n",
       "        def function_docstring():\n",
       "`DocStrings.function_docstring`s appear as native docstrings, but render as `markdown`.\n",
       "            \n",
       "            ...\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "    import doctest\n",
       "### `doctest`\n",
       "\n",
       "    >>> assert True\n",
       "    >>> print\n",
       "    <built-in function print>\n",
       "    >>> pidgy\n",
       "    <module...__init__.py'>\n",
       "\n",
       "\n",
       "\n",
       "### templating\n",
       "\n",
       "filters\n",
       "jinja docs\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-03T22:30:06.273636",
       "modules": [],
       "names": [],
       "start_time": "2020-02-03T22:30:06.161106"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## `pidgy` metasyntax at language interfaces.\n",
    "[üìó]({{pathlib.Path(test_pidgin_syntax.__file__).name}})\n",
    "\n",
    "The combinations of document, programming, and templating languages\n",
    "provides unique syntaxes as the interfaces.\n",
    "\n",
    "{{appendix.exports(test_pidgin_syntax)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Reusing `pidgy` documents.\n",
       "\n",
       "Notebooks gain value when they be reusable at rest.\n",
       "\n",
       "We'll make a cli application that deploys `pidgy` as a web, cli, converter.\n",
       "\n",
       "# `pidgy` command line application\n",
       "\n",
       "\n",
       "\n",
       "    import click, IPython, pidgy\n",
       "\n",
       "\n",
       "\n",
       "    @click.group()\n",
       "    def app(): \n",
       "The `pidgy` command line application operates on passive notebooks\n",
       "documents.\n",
       "\n",
       "\n",
       "\n",
       "    @app.group()\n",
       "    def kernel():\n",
       "Serve notebook modules from fastapi creating an openapi schema for each \n",
       "literate document.\n",
       "\n",
       "    @kernel.command()\n",
       "    def install(user=False, replace=None, prefix=None):\n",
       "        with pidgy.translate.pidgyLoader():\n",
       "            from .kernel import shell\n",
       "        dest =shell.install(user=user, replace=replace, prefix=prefix)\n",
       "        click.echo(F\"The pidgy kernel was install in {dest}\")\n",
       "        \n",
       "    @kernel.command()\n",
       "    def uninstall(user=True, replace=None, prefix=None):\n",
       "        with pidgy.translate.pidgyLoader():\n",
       "            from .kernel import shell\n",
       "        shell.uninstall()\n",
       "        click.echo(F\"The pidgy kernel was removed.\")\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    @app.command()\n",
       "    def serve(modules):\n",
       "Serve notebook modules from fastapi creating an openapi schema for each \n",
       "literate document.\n",
       "\n",
       "\n",
       "\n",
       "    @app.command()\n",
       "    def run(modules, parallel=True):\n",
       "Run a collection of notebook modules.\n",
       "\n",
       "\n",
       "\n",
       "    @app.command()\n",
       "    def convert(modules):\n",
       "Convert notebook written in pidgy to difference formats.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "text/markdown": {
       "end_time": "2020-02-03T22:29:40.813417",
       "modules": [],
       "names": [],
       "start_time": "2020-02-03T22:29:40.709700"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Reusing `pidgy` documents.\n",
    "\n",
    "Notebooks gain value when they be reusable at rest.\n",
    "\n",
    "We'll make a cli application that deploys `pidgy` as a web, cli, converter.\n",
    "\n",
    "{{appendix.exports(pidgy.cli)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "    \n",
    "    def unload_ipython_extension(shell):\n",
    "        for x in (exports, testing, imports): x.unload_ipython_extension(shell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook paper.md.ipynb to markdown\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "    if __name__ == '__main__':\n",
    "        !jupyter nbconvert --to markdown --TemplateExporter.exclude_input=True --stdout paper.md.ipynb > readme.md\n",
    "    ...;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pidgy 3",
   "language": "python",
   "name": "pidgy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
